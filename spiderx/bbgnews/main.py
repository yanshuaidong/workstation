#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Bloomberg æ–°é—»è‡ªåŠ¨å¤„ç†æœåŠ¡

åŠŸèƒ½è¯´æ˜ï¼š
1. æ¥æ”¶æµè§ˆå™¨æ‰©å±•å‘é€çš„æ–°é—»æ•°æ®ï¼ˆFlask APIï¼‰
2. å®æ—¶å­˜å‚¨åˆ°æœ¬åœ°SQLiteæ•°æ®åº“ bloomberg_news è¡¨
3. å®šæ—¶å¤„ç†æ–°é—»ï¼ˆæ¯å¤©6/12/18/24ç‚¹ï¼‰
4. AIç­›é€‰æœŸè´§ç›¸å…³æ–°é—»å¹¶æ ¼å¼åŒ–è¾“å‡º
5. ä¿å­˜åˆ°æœ¬åœ°SQLiteæ•°æ®åº“ analysis_task è¡¨
6. åˆ é™¤å·²å¤„ç†çš„æ–°é—»æ•°æ®

å·¥ä½œæµç¨‹ï¼š
æ’ä»¶å‘é€ -> bloomberg_newsè¡¨ -> å®šæ—¶è§¦å‘ -> AIç­›é€‰ -> analysis_taskè¡¨ -> åˆ é™¤å·²å¤„ç†æ•°æ®
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
from apscheduler.schedulers.background import BackgroundScheduler
import json
import os
import time
import sqlite3
import requests
import signal
import sys
from datetime import datetime, timedelta
from pathlib import Path
import logging

# ==================== é…ç½®éƒ¨åˆ† ====================

# Flaskåº”ç”¨é…ç½®
app = Flask(__name__)
CORS(app)

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('bloomberg_service.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# æ•°æ®åº“è·¯å¾„é…ç½®ï¼ˆä½¿ç”¨ spiderx/db ç›®å½•ä¸‹çš„æ•°æ®åº“ï¼‰
DB_DIR = Path(__file__).parent.parent / "db"
DB_PATH = DB_DIR / "crawler.db"

# AI API é…ç½®
AI_API_KEY = "sk-qVU4OZNspU5cSTPONFBFD000t2Oy8Tq9U8h74Wm5Phnl8tsB"
AI_BASE_URL = "https://poloai.top/v1/chat/completions"

# Bloomberg URLå‰ç¼€
BLOOMBERG_URL_PREFIX = "https://www.bloomberg.com"

# æœåŠ¡ç«¯å£
SERVICE_PORT = 1123

# ==================== æ•°æ®åº“æ“ä½œ ====================

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    if not DB_PATH.exists():
        logger.warning("âš ï¸ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ init_db.py")
        # å°è¯•åˆå§‹åŒ–æ•°æ®åº“
        sys.path.insert(0, str(DB_DIR))
        from init_db import init_db
        init_db()
    
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row  # æ”¯æŒå­—å…¸å¼è®¿é—®
    return conn


def save_news_to_db(news_item):
    """
    ä¿å­˜æ–°é—»åˆ° bloomberg_news è¡¨ï¼ˆå¸¦å»é‡ï¼‰
    
    å‚æ•°ï¼š
        news_item: æ–°é—»æ•°æ®å­—å…¸ï¼ŒåŒ…å« publishedAt, headline, brand, url
    
    è¿”å›ï¼š
        bool: æ˜¯å¦æˆåŠŸæ’å…¥ï¼ˆé‡å¤æ•°æ®è¿”å› Falseï¼‰
    """
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # è¡¥å…¨URLï¼ˆä½¿ç”¨ or ç¡®ä¿ None å€¼è¢«å¤„ç†ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
        url = news_item.get('url') or ''
        if url and not url.startswith('http'):
            url = BLOOMBERG_URL_PREFIX + url
        
        # è§£æå‘å¸ƒæ—¶é—´
        published_at = news_item.get('publishedAt') or ''
        
        # ä½¿ç”¨ INSERT OR IGNORE å®ç°å»é‡ï¼ˆåŸºäº published_at å”¯ä¸€ç´¢å¼•ï¼‰
        insert_sql = """
            INSERT OR IGNORE INTO bloomberg_news 
            (published_at, headline, brand, url, status)
            VALUES (?, ?, ?, ?, 0)
        """
        cursor.execute(insert_sql, (
            published_at,
            news_item.get('headline', ''),
            news_item.get('brand', ''),
            url
        ))
        
        conn.commit()
        
        # rowcount > 0 è¡¨ç¤ºæ’å…¥æˆåŠŸï¼Œ= 0 è¡¨ç¤ºé‡å¤æ•°æ®è¢«å¿½ç•¥
        return cursor.rowcount > 0
        
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜æ–°é—»åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        if conn:
            conn.rollback()
        return False
    finally:
        if conn:
            conn.close()


def get_pending_news(start_time, end_time):
    """
    è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…å¾…å¤„ç†çš„æ–°é—»ï¼ˆstatus=0ï¼‰
    
    å‚æ•°ï¼š
        start_time: å¼€å§‹æ—¶é—´ï¼ˆdatetimeå¯¹è±¡ï¼‰
        end_time: ç»“æŸæ—¶é—´ï¼ˆdatetimeå¯¹è±¡ï¼‰
    
    è¿”å›ï¼š
        list: æ–°é—»åˆ—è¡¨
    """
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # æ ¹æ® created_at ç­›é€‰æ—¶é—´èŒƒå›´å†…çš„æœªå¤„ç†æ–°é—»
        select_sql = """
            SELECT id, published_at, headline, brand, url
            FROM bloomberg_news
            WHERE status = 0 
            AND created_at >= ? 
            AND created_at < ?
            ORDER BY created_at ASC
        """
        cursor.execute(select_sql, (
            start_time.strftime('%Y-%m-%d %H:%M:%S'),
            end_time.strftime('%Y-%m-%d %H:%M:%S')
        ))
        
        rows = cursor.fetchall()
        return [dict(row) for row in rows]
        
    except Exception as e:
        logger.error(f"âŒ è·å–å¾…å¤„ç†æ–°é—»å¤±è´¥: {e}")
        return []
    finally:
        if conn:
            conn.close()


def mark_news_as_processed(news_ids):
    """
    å°†æŒ‡å®šæ–°é—»æ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆstatus=1ï¼‰
    
    å‚æ•°ï¼š
        news_ids: æ–°é—»IDåˆ—è¡¨
    """
    if not news_ids:
        return
    
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        placeholders = ','.join(['?' for _ in news_ids])
        update_sql = f"""
            UPDATE bloomberg_news 
            SET status = 1, updated_at = CURRENT_TIMESTAMP
            WHERE id IN ({placeholders})
        """
        cursor.execute(update_sql, news_ids)
        conn.commit()
        
        logger.info(f"âœ… å·²æ ‡è®° {len(news_ids)} æ¡æ–°é—»ä¸ºå·²å¤„ç†")
        
    except Exception as e:
        logger.error(f"âŒ æ ‡è®°æ–°é—»çŠ¶æ€å¤±è´¥: {e}")
        if conn:
            conn.rollback()
    finally:
        if conn:
            conn.close()


def delete_processed_news():
    """
    åˆ é™¤å·²å¤„ç†çš„æ–°é—»ï¼ˆstatus=1ï¼‰
    """
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # å…ˆç»Ÿè®¡è¦åˆ é™¤çš„æ•°é‡
        cursor.execute("SELECT COUNT(*) FROM bloomberg_news WHERE status = 1")
        count = cursor.fetchone()[0]
        
        if count == 0:
            logger.info("â„¹ï¸ æ²¡æœ‰éœ€è¦åˆ é™¤çš„å·²å¤„ç†æ–°é—»")
            return
        
        # åˆ é™¤å·²å¤„ç†çš„æ–°é—»
        delete_sql = "DELETE FROM bloomberg_news WHERE status = 1"
        cursor.execute(delete_sql)
        conn.commit()
        
        logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤ {count} æ¡å·²å¤„ç†æ–°é—»")
        
    except Exception as e:
        logger.error(f"âŒ åˆ é™¤å·²å¤„ç†æ–°é—»å¤±è´¥: {e}")
        if conn:
            conn.rollback()
    finally:
        if conn:
            conn.close()


def save_analysis_task(title, prompt, news_time):
    """
    ä¿å­˜åˆ†æä»»åŠ¡åˆ° analysis_task è¡¨
    
    å‚æ•°ï¼š
        title: ä»»åŠ¡æ ‡é¢˜
        prompt: æç¤ºè¯/åˆ†æå†…å®¹
        news_time: æ–°é—»æ—¶é—´ï¼ˆdatetimeå¯¹è±¡ï¼‰
    
    è¿”å›ï¼š
        int or None: æˆåŠŸè¿”å›ä»»åŠ¡IDï¼Œå¤±è´¥è¿”å›None
    """
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        insert_sql = """
            INSERT INTO analysis_task 
            (title, prompt, news_time, ai_result, is_analyzed)
            VALUES (?, ?, ?, '', 0)
        """
        cursor.execute(insert_sql, (
            title,
            prompt,
            news_time.strftime('%Y-%m-%d %H:%M:%S')
        ))
        
        task_id = cursor.lastrowid
        conn.commit()
        
        logger.info(f"âœ… åˆ†æä»»åŠ¡ä¿å­˜æˆåŠŸ - ä»»åŠ¡ID: {task_id}")
        return task_id
        
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜åˆ†æä»»åŠ¡å¤±è´¥: {e}")
        if conn:
            conn.rollback()
        return None
    finally:
        if conn:
            conn.close()


def get_pending_news_count():
    """è·å–å¾…å¤„ç†æ–°é—»æ•°é‡"""
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM bloomberg_news WHERE status = 0")
        return cursor.fetchone()[0]
    except Exception as e:
        logger.error(f"âŒ è·å–å¾…å¤„ç†æ–°é—»æ•°é‡å¤±è´¥: {e}")
        return 0
    finally:
        if conn:
            conn.close()


# ==================== AIæ¥å£è°ƒç”¨ ====================

def call_ai_api(news_list, max_retries=2):
    """
    è°ƒç”¨AIæ¥å£ç­›é€‰æ–°é—»ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    
    å‚æ•°ï¼š
        news_list: æ–°é—»åˆ—è¡¨ï¼Œæ¯æ¡åŒ…å« headline, brand, url
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤2æ¬¡ï¼‰
    
    è¿”å›ï¼š
        str or None: AIè¿”å›çš„ç­›é€‰ç»“æœï¼Œå¤±è´¥è¿”å›None
    """
    # æ„å»ºæç¤ºè¯
    news_json = json.dumps(news_list, ensure_ascii=False, indent=2)
    
    system_message = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœŸè´§å¸‚åœºæ–°é—»ç­›é€‰ä¸ç¿»è¯‘åŠ©æ‰‹ï¼ŒæœåŠ¡äºé‡åŒ–äº¤æ˜“ç³»ç»Ÿã€‚

ã€æ ¸å¿ƒç›®æ ‡ã€‘
ä»ç»™å®šæ–°é—»ä¸­ï¼Œåªä¿ç•™é‚£äº›å¯¹"å¯äº¤æ˜“æœŸè´§èµ„äº§"ä»·æ ¼åœ¨çŸ­æœŸå†…å¯èƒ½äº§ç”Ÿæ˜¾è‘—å½±å“çš„æ–°é—»ï¼Œå¹¶ç¿»è¯‘æˆä¸­æ–‡æ ‡é¢˜ã€‚
è‹¥å½±å“ä¸ç¡®å®šæˆ–æå¼±ï¼Œåº”å®å¯ä¸é€‰ã€‚

ã€ä¸¥æ ¼ç­›é€‰èŒƒå›´ï¼ˆå¿…é¡»ç›´æ¥å…³è”æœŸè´§äº¤æ˜“é©±åŠ¨ï¼‰ã€‘

ä¸€ã€åº”ä¿ç•™ï¼ˆæ»¡è¶³ä»»æ„ä¸€æ¡å³å¯ï¼‰ï¼š
1. å®è§‚æ”¿ç­–ä¸é£é™©äº‹ä»¶ï¼š
   - å¤®è¡Œå†³è®®ã€åˆ©ç‡å˜åŒ–ã€æ„å¤–æ”¿ç­–ä¿¡å·
   - å¤–æ±‡ç›‘ç®¡ã€å¹²é¢„æ”¿ç­–æˆ–æ±‡ç‡åˆ›å†å²æå€¼
   - é‡å¤§åˆ¶è£ã€æˆ˜äº‰ã€ç¦è¿ã€å…³ç¨ã€è´¸æ˜“æ”¿ç­–å˜åŠ¨

2. å½±å“å¤§å®—å•†å“ä¾›éœ€çš„äº‹ä»¶ï¼š
   - çŸ³æ²¹/å¤©ç„¶æ°”/OPECå†³è®®ã€å¤§å‹æ²¹ç”°æˆ–çŸ¿å±±åœäº§/å¢äº§/äº‹æ•…
   - å†œäº§å“å¤©æ°”å¼‚å¸¸ã€ç¾å®³ã€ç—…è™«å®³ã€å‡ºå£/è¿›å£å°é”æˆ–é‡‡è´­å–æ¶ˆ
   - é‡‘å±æˆ–èƒ½æºåº“å­˜æ˜¾è‘—å˜åŒ–ã€é™äº§ã€è¡¥åº“ã€å‚¨å¤‡é‡Šæ”¾

3. è¶…é¢„æœŸçš„å…³é”®å®è§‚æ•°æ®ï¼š
   - CPIã€PPIã€GDPã€PMIã€å°±ä¸šã€åº“å­˜ã€è´¸æ˜“æ•°æ®ç­‰
   - ä¸”æ–°é—»æ˜ç¡®è¡¨è¿°"è¶…é¢„æœŸ/ä¸åŠé¢„æœŸ/å¸‚åœºéœ‡åŠ¨/ä»·æ ¼ååº”"

4. è¯„çº§æœºæ„æˆ–å›½å®¶é£é™©äº‹ä»¶ï¼š
   - ä¸»æƒæˆ–å¤§å‹æœºæ„è¯„çº§è°ƒæ•´ã€è¿çº¦ã€æµåŠ¨æ€§å±æœº

5. é‡‘èå¸‚åœºæƒ…ç»ªé©±åŠ¨äº‹ä»¶ï¼š
   - å½±å“é£é™©åå¥½æˆ–æ³¢åŠ¨ç‡çš„çªå‘äº‹ä»¶ï¼ˆå¦‚ç³»ç»Ÿæ€§é‡‘èé£é™©ã€é‡å¤§è¿çº¦ã€é‡‘èæœºæ„å±æœºï¼‰

äºŒã€åº”å‰”é™¤ï¼ˆé™¤éæ–‡æœ¬è¯´æ˜å·²å¼•å‘æ˜¾è‘—ä»·æ ¼æ³¢åŠ¨ï¼‰ï¼š
1. å…¬å¸ç»è¥æ´»åŠ¨ï¼šå¹¶è´­ã€æ‰©äº§ã€åˆä½œã€æ–°é¡¹ç›®ç­‰æ™®é€šæ–°é—»
2. ç®¡ç†äººåŠ¨æ€ï¼šåŸºé‡‘å‘å”®ã€PEã€å¯¹å†²åŸºé‡‘å˜åŠ¨
3. ESGã€å“ç‰Œã€å…¬å…³ã€ç¤¾ä¼šè´£ä»»æŠ¥é“
4. é«˜ç®¡å˜æ›´æˆ–ä¼ä¸šæ²»ç†æ–°é—»
5. åŒºåŸŸé¡¹ç›®ã€åŸå¸‚å‘å±•ã€ä¸€èˆ¬åˆ¶é€ ä¸šæŠ•èµ„
6. è¡Œä¸šè§‚ç‚¹ã€é¢„æµ‹ã€ç ”ç©¶æŠ¥å‘Šã€å±•æœ›ç±»æè¿°

ä¸‰ã€å®¡æŸ¥é€»è¾‘ï¼š
- ç«™åœ¨"å¯äº¤æ˜“æœŸè´§å¸‚åœºï¼ˆå•†å“ã€è‚¡æŒ‡ã€å¤–æ±‡ã€åˆ©ç‡ç­‰ï¼‰äº¤æ˜“è€…"è§’åº¦è¯„ä¼°å†²å‡»æ€§ã€‚
- è‹¥æ²¡æœ‰æ˜æ˜¾å½±å“æˆ–æ–°é—»å†…å®¹åæ³›åŒ–ï¼Œç›´æ¥å¿½ç•¥ã€‚
- è‹¥ä¸ç¡®å®šå†²å‡»æ˜¯å¦æ˜¾è‘—ï¼Œåº”å®å¯ä¸é€‰ã€‚
- è‹¥æ‰€æœ‰æ–°é—»å‡æ— æœ‰æ•ˆå†²å‡»ï¼Œåº”è¾“å‡ºï¼šæ— é‡è¦ç›¸å…³æ–°é—»

ã€è¾“å‡ºæ ¼å¼è¦æ±‚ã€‘
1. å¯¹ä¿ç•™çš„æ–°é—»æ ‡é¢˜ç¿»è¯‘æˆç®€æ´è‡ªç„¶çš„ä¸­æ–‡ï¼Œå¹¶æ ‡æ˜ç›¸å…³æœŸè´§å“ç§ã€‚
2. æ ¼å¼ä¸¥æ ¼ä¸ºï¼ˆæ¯æ¡æ–°é—»ä¸€è¡Œï¼‰ï¼š
ã€XXæœŸè´§ç›¸å…³ã€‘ã€ç¿»è¯‘åçš„ä¸­æ–‡æ ‡é¢˜ã€‘æ–°é—»URL
3. XXä¸ºå…·ä½“æœŸè´§å“ç§ï¼Œå¦‚ï¼šåŸæ²¹ã€é»„é‡‘ã€é“œã€å¤§è±†ã€ç‰ç±³ã€è‚¡æŒ‡ã€å¤–æ±‡ç­‰ã€‚
4. ä¸ä¿ç•™ä»»ä½•è§£é‡Šã€è¯„è¯­æˆ–é¢å¤–å†…å®¹ã€‚
5. æ— ç¬¦åˆå†…å®¹æ—¶è¾“å‡ºï¼šæ— é‡è¦ç›¸å…³æ–°é—»
"""

    user_message = f"""è¯·ä»ä»¥ä¸‹æ–°é—»ä¸­ç­›é€‰å‡ºå¯¹æœŸè´§å¸‚åœºä»·æ ¼åœ¨çŸ­æœŸå†…å¯èƒ½äº§ç”Ÿæ˜æ˜¾å½±å“çš„å†…å®¹ï¼Œå¹¶æŒ‰è¦æ±‚æ ¼å¼è¾“å‡ºï¼š

{news_json}"""

    # é‡è¯•é…ç½®
    timeouts = [60, 120]  # ç¬¬ä¸€æ¬¡60ç§’ï¼Œç¬¬äºŒæ¬¡120ç§’
    
    for attempt in range(max_retries):
        try:
            timeout = timeouts[attempt] if attempt < len(timeouts) else timeouts[-1]
            logger.info(f"ğŸ¤– è°ƒç”¨AIæ¥å£ (ç¬¬{attempt + 1}æ¬¡å°è¯•ï¼Œè¶…æ—¶{timeout}ç§’)...")
            
            payload = {
                "model": "gpt-4.1-mini",
                "messages": [
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": user_message}
                ],
                "temperature": 0.7,
                "max_tokens": 5000
            }
            
            headers = {
                "Accept": "application/json",
                "Authorization": f"Bearer {AI_API_KEY}",
                "Content-Type": "application/json"
            }
            
            response = requests.post(AI_BASE_URL, json=payload, headers=headers, timeout=timeout)
            response.raise_for_status()
            
            result = response.json()["choices"][0]["message"]["content"]
            logger.info(f"âœ… AIæ¥å£è°ƒç”¨æˆåŠŸ (ç¬¬{attempt + 1}æ¬¡å°è¯•)")
            return result
            
        except Exception as e:
            logger.error(f"âŒ AIæ¥å£è°ƒç”¨å¤±è´¥ (ç¬¬{attempt + 1}æ¬¡å°è¯•): {e}")
            if attempt == max_retries - 1:
                logger.error("âŒ AIæ¥å£è°ƒç”¨æœ€ç»ˆå¤±è´¥ï¼Œå·²ç”¨å°½æ‰€æœ‰é‡è¯•æ¬¡æ•°")
                return None
            time.sleep(2)  # é‡è¯•å‰ç­‰å¾…2ç§’
    
    return None


# ==================== å®šæ—¶ä»»åŠ¡ ====================

def build_analysis_prompt(ai_result):
    """
    æ„å»ºåˆ†æä»»åŠ¡çš„æç¤ºè¯
    
    å‚æ•°ï¼š
        ai_result: AIç­›é€‰åçš„æ–°é—»å†…å®¹
    
    è¿”å›ï¼š
        str: å®Œæ•´çš„åˆ†ææç¤ºè¯
    """
    analysis_instruction = """
è¯·æ ¹æ®ä»¥ä¸Šæ–°é—»ä¿¡æ¯ï¼Œè¿›è¡Œæ·±åº¦åˆ†æå¹¶ç»™å‡ºå¯æ“ä½œçš„äº¤æ˜“å»ºè®®ï¼š

ã€åˆ†æè¦æ±‚ã€‘
1. å¸‚åœºå½±å“åˆ†æï¼š
   - åˆ†ææ¯æ¡æ–°é—»å¯¹ç›¸å…³æœŸè´§å“ç§çš„æ½œåœ¨å½±å“æ–¹å‘ï¼ˆåˆ©å¤š/åˆ©ç©ºï¼‰
   - è¯„ä¼°å½±å“çš„æ—¶æ•ˆæ€§ï¼ˆçŸ­æœŸ/ä¸­æœŸï¼‰å’Œå¼ºåº¦ï¼ˆå¼º/ä¸­/å¼±ï¼‰

2. äº¤æ˜“æœºä¼šè¯†åˆ«ï¼š
   - æ˜ç¡®æŒ‡å‡ºå¯èƒ½å­˜åœ¨çš„äº¤æ˜“æœºä¼š
   - ç»™å‡ºå»ºè®®çš„äº¤æ˜“æ–¹å‘ï¼ˆåšå¤š/åšç©ºï¼‰
   - è¯´æ˜å…¥åœºæ—¶æœºå’Œæ³¨æ„äº‹é¡¹

3. é£é™©æç¤ºï¼š
   - æŒ‡å‡ºå¯èƒ½çš„é£é™©å› ç´ 
   - ç»™å‡ºæ­¢æŸå»ºè®®æˆ–è§‚å¯Ÿè¦ç‚¹

4. å…³è”æ€§åˆ†æï¼š
   - åˆ†æä¸åŒå“ç§ä¹‹é—´çš„è”åŠ¨å…³ç³»
   - æŒ‡å‡ºå¯èƒ½çš„å¥—åˆ©æˆ–å¯¹å†²æœºä¼š

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·æŒ‰å“ç§åˆ†ç±»è¾“å‡ºåˆ†æç»“æœï¼Œæ¯ä¸ªå“ç§åŒ…å«ï¼šå½±å“åˆ†æã€äº¤æ˜“å»ºè®®ã€é£é™©æç¤ºã€‚
"""
    
    return f"{ai_result}\n\n{analysis_instruction}"


def process_news_task():
    """
    å®šæ—¶ä»»åŠ¡ï¼šå¤„ç†æ–°é—»
    åœ¨ 6/12/18/24 ç‚¹æ‰§è¡Œ
    """
    now = datetime.now()
    current_hour = now.hour
    
    # ç¡®å®šå¤„ç†çš„æ—¶é—´æ®µ
    if current_hour == 6:
        start_hour, end_hour = 0, 6
        time_label = "0ç‚¹åˆ°6ç‚¹"
    elif current_hour == 12:
        start_hour, end_hour = 6, 12
        time_label = "6ç‚¹åˆ°12ç‚¹"
    elif current_hour == 18:
        start_hour, end_hour = 12, 18
        time_label = "12ç‚¹åˆ°18ç‚¹"
    elif current_hour == 0:
        start_hour, end_hour = 18, 24
        time_label = "18ç‚¹åˆ°24ç‚¹"
    else:
        logger.warning(f"âš ï¸ éé¢„æœŸçš„æ‰§è¡Œæ—¶é—´: {current_hour}ç‚¹")
        return
    
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ• å¼€å§‹å¤„ç† {now.strftime('%Yå¹´%mæœˆ%dæ—¥')} {time_label} çš„æ–°é—»")
    logger.info(f"{'='*60}")
    
    # è®¡ç®—æ—¶é—´èŒƒå›´
    start_time = now.replace(hour=start_hour, minute=0, second=0, microsecond=0)
    if end_hour == 24:
        end_time = (start_time + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
    else:
        end_time = now.replace(hour=end_hour, minute=0, second=0, microsecond=0)
    
    # ä»æ•°æ®åº“è·å–å¾…å¤„ç†æ–°é—»
    target_news = get_pending_news(start_time, end_time)
    
    logger.info(f"ğŸ“Š æ‰¾åˆ° {len(target_news)} æ¡å¾…å¤„ç†æ–°é—»")
    
    if len(target_news) == 0:
        logger.info("â„¹ï¸ æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–°é—»")
        logger.info(f"{'='*60}\n")
        return
    
    # è·å–æ–°é—»IDåˆ—è¡¨ï¼Œç”¨äºåç»­æ ‡è®°
    news_ids = [item['id'] for item in target_news]
    
    # å‡†å¤‡å‘é€ç»™AIçš„æ–°é—»åˆ—è¡¨
    news_for_ai = [
        {
            'publishedAt': item.get('published_at'),
            'headline': item.get('headline'),
            'brand': item.get('brand', ''),
            'url': item.get('url', '')
        }
        for item in target_news
    ]
    
    # è°ƒç”¨AIæ¥å£ç­›é€‰
    ai_result = call_ai_api(news_for_ai)
    
    if ai_result is None:
        logger.error("âŒ AIç­›é€‰å¤±è´¥ï¼Œæœ¬æ¬¡ä»»åŠ¡ç»ˆæ­¢")
        logger.info(f"{'='*60}\n")
        return
    
    # æ£€æŸ¥æ˜¯å¦æ— é‡è¦æ–°é—»
    ai_result_stripped = ai_result.strip()
    if "æ— é‡è¦ç›¸å…³æ–°é—»" in ai_result_stripped:
        logger.info("â„¹ï¸ AIç­›é€‰ç»“æœï¼šæ— é‡è¦ç›¸å…³æ–°é—»ï¼Œè·³è¿‡å…¥åº“")
        # ä»ç„¶æ ‡è®°æ–°é—»ä¸ºå·²å¤„ç†å¹¶åˆ é™¤
        mark_news_as_processed(news_ids)
        delete_processed_news()
        logger.info("âœ… æ–°é—»å·²æ ‡è®°å¤„ç†å®Œæˆï¼ˆæ— éœ€åˆ›å»ºåˆ†æä»»åŠ¡ï¼‰")
        logger.info(f"{'='*60}\n")
        return
    
    # æ„å»ºæ ‡é¢˜
    date_str = now.strftime('%Yå¹´%mæœˆ%dæ—¥')
    title = f"ã€å½­åšç¤¾{date_str}{time_label}æ–°é—»ã€‘"
    
    # æ„å»ºå®Œæ•´çš„åˆ†ææç¤ºè¯
    prompt = build_analysis_prompt(ai_result_stripped)
    
    logger.info(f"ğŸ“ æ ‡é¢˜: {title}")
    logger.info(f"ğŸ“„ AIç­›é€‰ç»“æœé¢„è§ˆ: {ai_result_stripped[:200]}...")
    
    # ä¿å­˜åˆ° analysis_task è¡¨
    task_id = save_analysis_task(title, prompt, start_time)
    
    if task_id:
        # æ ‡è®°æ–°é—»ä¸ºå·²å¤„ç†
        mark_news_as_processed(news_ids)
        
        # åˆ é™¤å·²å¤„ç†çš„æ–°é—»
        delete_processed_news()
        
        logger.info("âœ… æ–°é—»å¤„ç†å®Œæˆ")
    else:
        logger.error("âŒ åˆ†æä»»åŠ¡ä¿å­˜å¤±è´¥ï¼Œä¸åˆ é™¤æ–°é—»æ•°æ®")
    
    logger.info(f"{'='*60}\n")


# ==================== Flaskè·¯ç”± ====================

@app.route('/api/capture', methods=['POST', 'OPTIONS'])
def capture_data():
    """æ¥æ”¶æµè§ˆå™¨æ‰©å±•å‘é€çš„æ–°é—»æ•°æ®"""
    
    if request.method == 'OPTIONS':
        return '', 200
    
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({
                'success': False,
                'message': 'æ²¡æœ‰æ¥æ”¶åˆ°æ•°æ®'
            }), 400
        
        # è·å–æ–°é—»åˆ—è¡¨
        captured_data = data.get('capturedData', [])
        
        # è°ƒè¯•ï¼šæ‰“å°ç¬¬ä¸€æ¡æ•°æ®æŸ¥çœ‹ç»“æ„
        if captured_data:
            logger.info(f"ğŸ” è°ƒè¯• - ç¬¬ä¸€æ¡åŸå§‹æ•°æ®: {captured_data[0]}")
        
        if not captured_data:
            return jsonify({
                'success': False,
                'message': 'æ•°æ®åˆ—è¡¨ä¸ºç©º'
            }), 400
        
        # æ·»åŠ æ–°é—»åˆ°æ•°æ®åº“
        added_count = 0
        for item in captured_data:
            brand = (item.get('brand') or '').lower().strip()
            news_item = {
                'publishedAt': item.get('publishedAt'),
                'headline': item.get('headline'),
                'brand': brand,
                'url': item.get('url') or ''  # ä½¿ç”¨ or å¤„ç† None å€¼
            }
            
            if save_news_to_db(news_item):
                added_count += 1
        
        # è·å–å½“å‰å¾…å¤„ç†æ€»æ•°
        total_count = get_pending_news_count()
        
        logger.info(f'âœ… æ–°é—»æ¥æ”¶æˆåŠŸ - æ–°å¢: {added_count} æ¡ | å¾…å¤„ç†æ€»è®¡: {total_count} æ¡')
        
        return jsonify({
            'success': True,
            'message': 'æ•°æ®ä¿å­˜æˆåŠŸ',
            'added': added_count,
            'total': total_count
        }), 200
        
    except Exception as e:
        logger.error(f'âŒ æ•°æ®æ¥æ”¶å¤±è´¥: {e}')
        return jsonify({
            'success': False,
            'message': f'ä¿å­˜å¤±è´¥: {str(e)}'
        }), 500


@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    pending_count = get_pending_news_count()
    return jsonify({
        'status': 'ok',
        'service': 'Bloombergæ–°é—»å¤„ç†æœåŠ¡',
        'port': SERVICE_PORT,
        'time': datetime.now().isoformat(),
        'pending_news': pending_count,
        'database': str(DB_PATH)
    })


@app.route('/api/stats', methods=['GET'])
def get_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # å¾…å¤„ç†æ–°é—»æ•°é‡
        cursor.execute("SELECT COUNT(*) FROM bloomberg_news WHERE status = 0")
        pending = cursor.fetchone()[0]
        
        # å·²å¤„ç†æ–°é—»æ•°é‡
        cursor.execute("SELECT COUNT(*) FROM bloomberg_news WHERE status = 1")
        processed = cursor.fetchone()[0]
        
        # åˆ†æä»»åŠ¡æ•°é‡
        cursor.execute("SELECT COUNT(*) FROM analysis_task")
        tasks = cursor.fetchone()[0]
        
        # å¾…åˆ†æä»»åŠ¡æ•°é‡
        cursor.execute("SELECT COUNT(*) FROM analysis_task WHERE is_analyzed = 0")
        pending_tasks = cursor.fetchone()[0]
        
        return jsonify({
            'bloomberg_news': {
                'pending': pending,
                'processed': processed,
                'total': pending + processed
            },
            'analysis_task': {
                'total': tasks,
                'pending': pending_tasks,
                'analyzed': tasks - pending_tasks
            },
            'database': str(DB_PATH)
        })
    except Exception as e:
        return jsonify({
            'error': str(e)
        }), 500
    finally:
        if conn:
            conn.close()


@app.route('/api/process_now', methods=['POST'])
def process_now():
    """æ‰‹åŠ¨è§¦å‘å¤„ç†ä»»åŠ¡ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    try:
        process_news_task()
        return jsonify({
            'success': True,
            'message': 'å¤„ç†ä»»åŠ¡å·²æ‰§è¡Œ'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': str(e)
        }), 500


def process_all_pending_news_for_test():
    """
    æµ‹è¯•ç”¨ï¼šå¤„ç†æ‰€æœ‰å¾…å¤„ç†çš„æ–°é—»ï¼ˆä¸å—æ—¶é—´é™åˆ¶ï¼‰
    """
    now = datetime.now()
    
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ§ª [æµ‹è¯•æ¨¡å¼] å¼€å§‹å¤„ç†æ‰€æœ‰å¾…å¤„ç†æ–°é—»")
    logger.info(f"{'='*60}")
    
    # è·å–æ‰€æœ‰å¾…å¤„ç†çš„æ–°é—»ï¼ˆä¸é™æ—¶é—´èŒƒå›´ï¼‰
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        select_sql = """
            SELECT id, published_at, headline, brand, url
            FROM bloomberg_news
            WHERE status = 0
            ORDER BY created_at ASC
        """
        cursor.execute(select_sql)
        rows = cursor.fetchall()
        target_news = [dict(row) for row in rows]
        
    except Exception as e:
        logger.error(f"âŒ è·å–å¾…å¤„ç†æ–°é—»å¤±è´¥: {e}")
        return {'success': False, 'message': f'è·å–æ–°é—»å¤±è´¥: {str(e)}', 'processed': 0}
    finally:
        if conn:
            conn.close()
    
    logger.info(f"ğŸ“Š æ‰¾åˆ° {len(target_news)} æ¡å¾…å¤„ç†æ–°é—»")
    
    if len(target_news) == 0:
        logger.info("â„¹ï¸ æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–°é—»")
        logger.info(f"{'='*60}\n")
        return {'success': True, 'message': 'æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–°é—»', 'processed': 0}
    
    # è·å–æ–°é—»IDåˆ—è¡¨
    news_ids = [item['id'] for item in target_news]
    
    # å‡†å¤‡å‘é€ç»™AIçš„æ–°é—»åˆ—è¡¨
    news_for_ai = [
        {
            'publishedAt': item.get('published_at'),
            'headline': item.get('headline'),
            'brand': item.get('brand', ''),
            'url': item.get('url', '')
        }
        for item in target_news
    ]
    
    # è°ƒç”¨AIæ¥å£ç­›é€‰
    ai_result = call_ai_api(news_for_ai)
    
    if ai_result is None:
        logger.error("âŒ AIç­›é€‰å¤±è´¥ï¼Œæœ¬æ¬¡ä»»åŠ¡ç»ˆæ­¢")
        logger.info(f"{'='*60}\n")
        return {'success': False, 'message': 'AIç­›é€‰å¤±è´¥', 'processed': 0}
    
    # æ£€æŸ¥æ˜¯å¦æ— é‡è¦æ–°é—»
    ai_result_stripped = ai_result.strip()
    if "æ— é‡è¦ç›¸å…³æ–°é—»" in ai_result_stripped:
        logger.info("â„¹ï¸ AIç­›é€‰ç»“æœï¼šæ— é‡è¦ç›¸å…³æ–°é—»ï¼Œè·³è¿‡å…¥åº“")
        # ä»ç„¶æ ‡è®°æ–°é—»ä¸ºå·²å¤„ç†å¹¶åˆ é™¤
        mark_news_as_processed(news_ids)
        delete_processed_news()
        logger.info("âœ… æµ‹è¯•å¤„ç†å®Œæˆï¼ˆæ— é‡è¦æ–°é—»ï¼Œæœªåˆ›å»ºåˆ†æä»»åŠ¡ï¼‰")
        logger.info(f"{'='*60}\n")
        return {
            'success': True, 
            'message': 'æ— é‡è¦ç›¸å…³æ–°é—»ï¼Œå·²æ¸…ç†åŸå§‹æ•°æ®', 
            'processed': len(news_ids),
            'task_id': None
        }
    
    # æ„å»ºæ ‡é¢˜ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰
    date_str = now.strftime('%Yå¹´%mæœˆ%dæ—¥')
    time_str = now.strftime('%H:%M')
    title = f"ã€å½­åšç¤¾{date_str} {time_str} æµ‹è¯•ã€‘"
    
    # æ„å»ºå®Œæ•´çš„åˆ†ææç¤ºè¯
    prompt = build_analysis_prompt(ai_result_stripped)
    
    logger.info(f"ğŸ“ æ ‡é¢˜: {title}")
    logger.info(f"ğŸ“„ AIç­›é€‰ç»“æœé¢„è§ˆ: {ai_result_stripped[:200]}...")
    
    # ä¿å­˜åˆ° analysis_task è¡¨
    task_id = save_analysis_task(title, prompt, now)
    
    if task_id:
        # æ ‡è®°æ–°é—»ä¸ºå·²å¤„ç†
        mark_news_as_processed(news_ids)
        
        # åˆ é™¤å·²å¤„ç†çš„æ–°é—»
        delete_processed_news()
        
        logger.info("âœ… æµ‹è¯•å¤„ç†å®Œæˆ")
        logger.info(f"{'='*60}\n")
        return {
            'success': True, 
            'message': f'å¤„ç†å®Œæˆï¼Œå·²åˆ›å»ºåˆ†æä»»åŠ¡ ID: {task_id}', 
            'processed': len(news_ids),
            'task_id': task_id
        }
    else:
        logger.error("âŒ åˆ†æä»»åŠ¡ä¿å­˜å¤±è´¥")
        logger.info(f"{'='*60}\n")
        return {'success': False, 'message': 'åˆ†æä»»åŠ¡ä¿å­˜å¤±è´¥', 'processed': 0}


@app.route('/api/process_test', methods=['POST'])
def process_test():
    """
    æµ‹è¯•æ¥å£ï¼šç«‹å³å¤„ç†æ‰€æœ‰å¾…å¤„ç†æ–°é—»ï¼ˆä¸å—æ—¶é—´é™åˆ¶ï¼‰
    ç”¨äºæµ‹è¯•ï¼Œæ— éœ€ç­‰å¾…å®šæ—¶ä»»åŠ¡çš„å…·ä½“æ—¶é—´
    """
    try:
        result = process_all_pending_news_for_test()
        return jsonify(result), 200 if result['success'] else 500
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤„ç†å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'message': str(e),
            'processed': 0
        }), 500


# ==================== ä¸»ç¨‹åº ====================

# å…¨å±€å˜é‡
scheduler = None
shutdown_flag = False


def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨ï¼Œç”¨äºä¼˜é›…é€€å‡º"""
    global shutdown_flag
    signal_name = signal.Signals(signum).name
    logger.info(f"æ”¶åˆ°ä¿¡å· {signal_name}ï¼Œå‡†å¤‡ä¼˜é›…é€€å‡º...")
    print(f"\nğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å· {signal_name}ï¼Œæ­£åœ¨å®‰å…¨åœæ­¢æœåŠ¡...")
    
    shutdown_flag = True
    
    # åœæ­¢è°ƒåº¦å™¨
    if scheduler:
        logger.info("æ­£åœ¨åœæ­¢å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨...")
        scheduler.shutdown(wait=False)
        logger.info("å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢")
    
    logger.info("æœåŠ¡å·²å®‰å…¨åœæ­¢")
    sys.exit(0)


if __name__ == '__main__':
    logger.info('='*60)
    logger.info('ğŸš€ Bloombergæ–°é—»å¤„ç†æœåŠ¡å¯åŠ¨')
    logger.info(f'ğŸ“ ç›‘å¬ç«¯å£: {SERVICE_PORT}')
    logger.info(f'ğŸ’¾ æ•°æ®åº“è·¯å¾„: {DB_PATH.absolute()}')
    logger.info(f'ğŸ”— æ¥æ”¶æ¥å£: http://localhost:{SERVICE_PORT}/api/capture')
    logger.info(f'ğŸ’š å¥åº·æ£€æŸ¥: http://localhost:{SERVICE_PORT}/api/health')
    logger.info(f'ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: http://localhost:{SERVICE_PORT}/api/stats')
    logger.info('='*60)
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)   # Ctrl+C
    signal.signal(signal.SIGTERM, signal_handler)  # killå‘½ä»¤
    logger.info("âœ… ä¿¡å·å¤„ç†å™¨å·²æ³¨å†Œ")
    
    # å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
    scheduler = BackgroundScheduler()
    
    # æ·»åŠ å®šæ—¶ä»»åŠ¡ï¼šæ¯å¤©çš„ 6ã€12ã€18ã€0 ç‚¹æ‰§è¡Œ
    scheduler.add_job(process_news_task, 'cron', hour='6,12,18,0', minute=0)
    
    scheduler.start()
    logger.info('â° å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ (æ¯å¤©6ç‚¹ã€12ç‚¹ã€18ç‚¹ã€24ç‚¹æ‰§è¡Œ)')
    
    # æ‰“å°ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´
    jobs = scheduler.get_jobs()
    if jobs:
        next_run_time = jobs[0].next_run_time
        logger.info(f'ğŸ“… ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´: {next_run_time.strftime("%Y-%m-%d %H:%M:%S")}')
    
    try:
        # å¯åŠ¨FlaskæœåŠ¡
        app.run(host='0.0.0.0', port=SERVICE_PORT, debug=False, use_reloader=False)
    except (KeyboardInterrupt, SystemExit):
        if scheduler:
            scheduler.shutdown()
        logger.info('ğŸ‘‹ æœåŠ¡å·²åœæ­¢')
